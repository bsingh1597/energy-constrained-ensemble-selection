{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22b5df4",
   "metadata": {},
   "source": [
    "# Energy Diversity Based Ensemble Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b34aa",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from itertools import combinations\n",
    "\n",
    "sys.path.append(\"/home/myid/bs83243/mastersProject/EnsembleBench/EnsembleBench/frameworks\")\n",
    "\n",
    "from pytorchUtility import (\n",
    "    calAccuracy,\n",
    "    calAveragePredictionVectorAccuracy,\n",
    "    filterModelsFixed,\n",
    ")\n",
    "\n",
    "from EnsembleBench.groupMetrics import(\n",
    "    calAllDiversityMetrics,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dae598",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Define dataset, models, paths, energy constraints, and other parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Configuration ---\n",
    "DATASET = 'imagenet' \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PREDICTION_SUFFIX = '.pt'\n",
    "\n",
    "PREDICTION_DIR = '/home/myid/bs83243/mastersProject/energy_constraint_ensemble/ValFileSorted/imagenet'\n",
    "MODELS = np.array(['densenet201', 'resnet18', 'resnet152', 'resnext50', 'vgg19_bn', \n",
    "                       'efficientnet_b0', 'inception_v3', 'squeezeNet1_1', 'alexnet', 'vgg16'])\n",
    "ENERGY_PROFILES = {\n",
    "    0: 27.457, # densenet201\n",
    "    1: 10.717, # resnet18\n",
    "    2: 29.823, # resnet152\n",
    "    3: 28.351, # resnext50\n",
    "    4: 49.646, # vgg19_bn\n",
    "    5: 8.794,  # efficientnet_b0\n",
    "    6: 32.011, # inception_v3\n",
    "    7: 10.646, # squeezeNet1_1\n",
    "    8: 11.201, # alexnet\n",
    "    9: 36.080  # vgg16\n",
    "}\n",
    "\n",
    "# --- Diversity Metrics ---\n",
    "DIVERSITY_METRICS_LIST = ['CK', 'QS', 'BD', 'FK', 'KW', 'GD']\n",
    "\n",
    "# --- Energy Constraint ---\n",
    "TOTAL_ENERGY = sum(ENERGY_PROFILES.values())\n",
    "ENERGY_CONSTRAINT_PERCENT = 50 # Set desired percentage\n",
    "ENERGY_CONSTRAINT = TOTAL_ENERGY * (ENERGY_CONSTRAINT_PERCENT / 100.0)\n",
    "\n",
    "# --- Diversity Calculation Settings ---\n",
    "CROSS_VALIDATION = True\n",
    "CROSS_VALIDATION_TIMES = 3\n",
    "N_RANDOM_SAMPLES = 1000\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Dataset: {DATASET}\")\n",
    "print(f\"  Models: {MODELS}\")\n",
    "print(f\"  Prediction Dir: {PREDICTION_DIR}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Energy Constraint: {ENERGY_CONSTRAINT:.3f} kWh ({ENERGY_CONSTRAINT_PERCENT}% of Total {TOTAL_ENERGY:.3f} kWh)\")\n",
    "print(f\"  Diversity Metrics: {DIVERSITY_METRICS_LIST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35eb24",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bfb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_team_accuracies(team_list, team_accuracy_dict, description=\"Analysis\"):\n",
    "    \"\"\"Analyzes and prints accuracy statistics for a list of teams.\"\"\"\n",
    "    if not team_list:\n",
    "        print(f\"{description}: No teams provided for analysis.\")\n",
    "        return None, 0.0, 0.0, 0.0, 0.0\n",
    "        \n",
    "    # Create a dictionary mapping the team to its accuracy for the provided list\n",
    "    filtered_accuracy_mapping = {}\n",
    "    for team in team_list:\n",
    "        team_key = ''.join(map(str, team))\n",
    "        if team_key in team_accuracy_dict:\n",
    "             filtered_accuracy_mapping[team_key] = team_accuracy_dict[team_key]\n",
    "        else:\n",
    "            print(f\"Warning: Team {team_key} not found in team_accuracy_dict.\")\n",
    "            \n",
    "    if not filtered_accuracy_mapping:\n",
    "        print(f\"{description}: No valid teams found in accuracy dictionary.\")\n",
    "        return None, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    accuracies = list(filtered_accuracy_mapping.values())\n",
    "    max_team_key = max(filtered_accuracy_mapping, key=filtered_accuracy_mapping.get)\n",
    "    min_team_key = min(filtered_accuracy_mapping, key=filtered_accuracy_mapping.get)\n",
    "    max_accuracy = filtered_accuracy_mapping[max_team_key]\n",
    "    min_accuracy = filtered_accuracy_mapping[min_team_key]\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    median_accuracy = np.median(accuracies)\n",
    "\n",
    "    print(f\"--- {description} --- ({len(team_list)} teams)\")\n",
    "    print(f\"  Max Accuracy Team: {max_team_key} with Accuracy: {max_accuracy:.3f}\")\n",
    "    print(f\"  Min Accuracy Team: {min_team_key} with Accuracy: {min_accuracy:.3f}\")\n",
    "    print(f\"  Mean Accuracy: {mean_accuracy:.3f}\")\n",
    "    print(f\"  Median Accuracy: {median_accuracy:.3f}\")\n",
    "    print(f\"-----------------------\")\n",
    "    return max_team_key, max_accuracy, min_accuracy, mean_accuracy, median_accuracy\n",
    "\n",
    "def is_team_within_energy_constraint(team, energy_profile_dict, energy_constraint):\n",
    "    \"\"\"Checks if a team's energy consumption is within the constraint.\"\"\"\n",
    "    try:\n",
    "        total_energy = sum(energy_profile_dict[model_idx] for model_idx in team)\n",
    "        return total_energy <= energy_constraint\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Model index {e} not found in energy profiles.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ca443",
   "metadata": {},
   "source": [
    "## 4. Core Logic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ff869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(prediction_dir, models, suffix, device):\n",
    "    \"\"\"Loads prediction and label vectors for given models.\"\"\"\n",
    "    print(\"Loading predictions...\")\n",
    "    labelVectorsList = []\n",
    "    predictionVectorsList = []\n",
    "    individualAccuracies = []\n",
    "    model_load_times = {}\n",
    "    \n",
    "    start_total_time = timeit.default_timer()\n",
    "    for i, m in enumerate(models):\n",
    "        start_model_time = timeit.default_timer()\n",
    "        predictionPath = os.path.join(prediction_dir, m + suffix)\n",
    "        try:\n",
    "            prediction = torch.load(predictionPath, map_location=device)\n",
    "            predictionVectors = prediction['predictionVectors']\n",
    "            labelVectors = prediction['labelVectors']\n",
    "            \n",
    "            # Apply softmax and move to CPU\n",
    "            predictionVectorsList.append(torch.nn.functional.softmax(predictionVectors, dim=-1).cpu())\n",
    "            labelVectorsList.append(labelVectors.cpu())\n",
    "            \n",
    "            # Calculate individual accuracy\n",
    "            acc = calAccuracy(predictionVectors, labelVectors)[0].cpu().item()\n",
    "            individualAccuracies.append(acc)\n",
    "            \n",
    "            end_model_time = timeit.default_timer()\n",
    "            model_load_times[m] = end_model_time - start_model_time\n",
    "            print(f\"  Loaded {m}: Accuracy={acc:.4f}, Time={model_load_times[m]:.2f}s\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Prediction file not found at {predictionPath}\")\n",
    "            # Handle error appropriately - skip model, raise exception, etc.\n",
    "            return None, None, None, None # Indicate failure\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Key {e} not found in prediction file {predictionPath}\")\n",
    "            return None, None, None, None\n",
    "            \n",
    "    end_total_time = timeit.default_timer()\n",
    "    print(f\"Finished loading predictions. Total time: {end_total_time - start_total_time:.2f}s\")\n",
    "    \n",
    "    if not labelVectorsList:\n",
    "        print(\"Error: No predictions were loaded successfully.\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    # Basic validation\n",
    "    print(f\"\\nLoaded {len(predictionVectorsList)} models.\")\n",
    "    print(f\"  Prediction vector size (first model): {predictionVectorsList[0].size()}\")\n",
    "    print(f\"  Label vector size: {labelVectorsList[0].size()}\")\n",
    "    print(f\"  Individual Accuracies: {[f'{acc:.4f}' for acc in individualAccuracies]}\")\n",
    "    print(f\"  Min/Avg/Max Accuracy: {np.min(individualAccuracies):.4f} / {np.mean(individualAccuracies):.4f} / {np.max(individualAccuracies):.4f}\")\n",
    "    \n",
    "    return predictionVectorsList, labelVectorsList[0], individualAccuracies, model_load_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_team_accuracies(prediction_vectors_list, label_vectors, num_models):\n",
    "    \"\"\"Calculates accuracy for all possible team combinations (soft voting).\"\"\"\n",
    "    print(\"\\nCalculating team accuracies (soft voting)...\")\n",
    "    teamAccuracyDict = {}\n",
    "    all_teams = []\n",
    "    startTime = timeit.default_timer()\n",
    "    \n",
    "    for n in range(2, num_models + 1):\n",
    "        comb = combinations(list(range(num_models)), n)\n",
    "        count = 0\n",
    "        for selectedModels in list(comb):\n",
    "            tmpAccuracy = calAveragePredictionVectorAccuracy(prediction_vectors_list, label_vectors, modelsList=selectedModels)[0].cpu().item()\n",
    "            teamName = \"\".join(map(str, selectedModels))\n",
    "            teamAccuracyDict[teamName] = tmpAccuracy\n",
    "            all_teams.append(selectedModels) # Store the tuple of indices\n",
    "            count += 1\n",
    "        print(f\"  Calculated accuracy for {count} teams of size {n}\")\n",
    "            \n",
    "    endTime = timeit.default_timer()\n",
    "    print(f\"Finished calculating team accuracies. Total time: {endTime - startTime:.2f}s\")\n",
    "    print(f\"  Total number of teams (size >= 2): {len(teamAccuracyDict)}\")\n",
    "    # analyze_team_accuracies(all_teams, teamAccuracyDict, \"Overall Team Accuracy Analysis\") # Optional: Analyze here\n",
    "    return teamAccuracyDict, all_teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2081038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_disagreement_samples(prediction_vectors_list, label_vectors):\n",
    "    \"\"\"Identifies samples where models disagree on the prediction.\"\"\"\n",
    "    print(\"\\nCalculating disagreement samples...\")\n",
    "    startTime = timeit.default_timer()\n",
    "    \n",
    "    target = label_vectors # Assuming label_vectors is already the target tensor\n",
    "    batchSize = target.size(0)\n",
    "    predictionList = []\n",
    "    \n",
    "    # Get predicted class index for each model\n",
    "    for pVL in prediction_vectors_list:\n",
    "        _, pred = pVL.max(dim=1)\n",
    "        predictionList.append(pred)\n",
    "        \n",
    "    sampleID = []\n",
    "    sampleTarget = []\n",
    "    predictions = []\n",
    "    predVectors = []\n",
    "    disagreement_count = 0\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        model_preds_for_sample = [p[i].item() for p in predictionList]\n",
    "        # Check if all predictions are the same\n",
    "        if len(set(model_preds_for_sample)) > 1:\n",
    "            disagreement_count += 1\n",
    "            sampleID.append(i)\n",
    "            sampleTarget.append(target[i].item())\n",
    "            predictions.append(model_preds_for_sample)\n",
    "            # Store prediction vectors (probabilities) for disagreed samples\n",
    "            predVectors.append([pv[i].numpy() for pv in prediction_vectors_list]) # Convert to numpy here if needed later\n",
    "            \n",
    "    endTime = timeit.default_timer()\n",
    "    print(f\"Finished calculating disagreement samples. Total time: {endTime - startTime:.2f}s\")\n",
    "    print(f\"  Found {disagreement_count} disagreement samples out of {batchSize}.\")\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    sampleID = np.array(sampleID)\n",
    "    sampleTarget = np.array(sampleTarget)\n",
    "    predictions = np.array(predictions)\n",
    "    # Ensure predVectors is a NumPy array of the correct shape if needed by downstream functions\n",
    "    # This conversion might be memory-intensive for large datasets/vectors\n",
    "    try:\n",
    "        predVectors = np.array(predVectors) \n",
    "    except ValueError as e:\n",
    "         print(f\"Warning: Could not convert predVectors to a uniform NumPy array: {e}. Keeping as list of lists.\")\n",
    "         # Handle non-uniform shapes if necessary\n",
    "         pass \n",
    "\n",
    "    return sampleID, sampleTarget, predictions, predVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diversity_scores(all_teams, sampleID, sampleTarget, predictions, predVectors, \n",
    "                               diversity_metrics_list, cross_validation=True, \n",
    "                               cross_validation_times=3, n_random_samples=1000):\n",
    "    \"\"\"Calculates diversity scores for all provided teams.\"\"\"\n",
    "    print(\"\\nCalculating diversity scores...\")\n",
    "    np.random.seed(0) # for reproducibility\n",
    "    diversityScoresList = []\n",
    "    teamSizeList = []\n",
    "    teamListProcessed = [] # Keep track of teams for which scores are calculated\n",
    "    nModels = predictions.shape[1] # Total number of base models\n",
    "    modelIdx = list(range(nModels))\n",
    "    \n",
    "    startTime = timeit.default_timer()\n",
    "    \n",
    "    processed_count = 0\n",
    "    for selectedModels in all_teams: # Iterate through the teams generated earlier\n",
    "        n = len(selectedModels)\n",
    "        # Filter disagreement data for the current team\n",
    "        # Note: filterModelsFixed expects predictions/predVectors for *all* models\n",
    "        teamSampleID, teamSampleTarget, teamPredictions, teamPredVectors = filterModelsFixed(\n",
    "            sampleID, sampleTarget, predictions, predVectors, selectedModels\n",
    "        )\n",
    "        \n",
    "        if len(teamPredictions) == 0:\n",
    "            print(f\"Warning: No disagreement samples found for team {selectedModels}. Skipping diversity calculation.\")\n",
    "            continue # Skip this team if no disagreements\n",
    "        \n",
    "        if cross_validation:\n",
    "            if len(teamPredictions) < n_random_samples:\n",
    "                # print(f\"Warning: Team {selectedModels} has fewer samples ({len(teamPredictions)}) than n_random_samples ({n_random_samples}). Using all samples.\")\n",
    "                current_n_random_samples = len(teamPredictions)\n",
    "            else:\n",
    "                current_n_random_samples = n_random_samples\n",
    "                \n",
    "            tmpMetrics = []\n",
    "            for _ in range(cross_validation_times):\n",
    "                if current_n_random_samples > 0:\n",
    "                    randomIdx = np.random.choice(np.arange(teamPredictions.shape[0]), current_n_random_samples, replace=False)\n",
    "                    tmpMetrics.append(calAllDiversityMetrics(teamPredictions[randomIdx], teamSampleTarget[randomIdx], diversity_metrics_list))\n",
    "                else:\n",
    "                     tmpMetrics.append([np.nan] * len(diversity_metrics_list)) # Handle case with 0 samples\n",
    "            # Handle potential NaNs if a metric calculation failed\n",
    "            tmpMetrics = np.nanmean(np.array(tmpMetrics), axis=0) \n",
    "        else:\n",
    "            tmpMetrics = np.array(calAllDiversityMetrics(teamPredictions, teamSampleTarget, diversity_metrics_list))\n",
    "        \n",
    "        diversityScoresList.append(tmpMetrics)\n",
    "        teamSizeList.append(n)\n",
    "        teamListProcessed.append(selectedModels)\n",
    "        processed_count += 1\n",
    "        if processed_count % 100 == 0:\n",
    "             print(f\"  Processed diversity for {processed_count}/{len(all_teams)} teams...\")\n",
    "            \n",
    "    endTime = timeit.default_timer()\n",
    "    print(f\"Finished calculating diversity scores. Total time: {endTime - startTime:.2f}s\")\n",
    "    print(f\"  Calculated scores for {len(teamListProcessed)} teams.\")\n",
    "    \n",
    "    diversityScoresList = np.array(diversityScoresList)\n",
    "    teamSizeList = np.array(teamSizeList)\n",
    "    teamListProcessed = np.array(teamListProcessed, dtype=object)\n",
    "    \n",
    "    return diversityScoresList, teamSizeList, teamListProcessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_team_diversity_dict(team_list, diversity_scores_list):\n",
    "    \"\"\"Creates a dictionary mapping team string to its diversity scores array.\"\"\"\n",
    "    team_diversity_dict = {}\n",
    "    if len(team_list) != len(diversity_scores_list):\n",
    "        print(\"Error: team_list and diversity_scores_list must have the same length.\")\n",
    "        return None\n",
    "    for i, team in enumerate(team_list):\n",
    "        team_key = ''.join(map(str, team))\n",
    "        team_diversity_dict[team_key] = diversity_scores_list[i]\n",
    "    print(f\"\\nCreated team-to-diversity dictionary with {len(team_diversity_dict)} entries.\")\n",
    "    return team_diversity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efb167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustive_search_constrained(all_teams, team_accuracy_dict, energy_profile_dict, energy_constraint):\n",
    "    \"\"\"Performs exhaustive search for the best team under energy constraints.\"\"\"\n",
    "    print(f\"\\nPerforming Exhaustive Search with Energy Constraint <= {energy_constraint:.3f} kWh...\")\n",
    "    startTime = timeit.default_timer()\n",
    "    \n",
    "    filtered_team_list = []\n",
    "    for team in all_teams:\n",
    "        if is_team_within_energy_constraint(team, energy_profile_dict, energy_constraint):\n",
    "            filtered_team_list.append(team)\n",
    "            \n",
    "    endTime = timeit.default_timer()\n",
    "    print(f\"Finished filtering teams. Time: {endTime - startTime:.2f}s\")\n",
    "    print(f\"  Found {len(filtered_team_list)} teams within the energy constraint.\")\n",
    "    \n",
    "    # Analyze the filtered teams\n",
    "    best_team_key, max_acc, _, _, _ = analyze_team_accuracies(\n",
    "        filtered_team_list, \n",
    "        team_accuracy_dict, \n",
    "        description=f\"Exhaustive Search (Constraint: {energy_constraint:.3f} kWh)\"\n",
    "    )\n",
    "    \n",
    "    return filtered_team_list, best_team_key, max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eac807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is just for testing to find the max diversity\n",
    "def greedy_search_max_diversity(all_teams, team_size_list, team_diversity_dict, energy_profile_dict, \n",
    "                                energy_constraint, diversity_metric_index, diversity_metrics_list):\n",
    "    \"\"\"Performs greedy search selecting teams by maximizing (or minimizing) a specific diversity metric.\"\"\"\n",
    "    metric_name = diversity_metrics_list[diversity_metric_index]\n",
    "    print(f\"\\nPerforming Greedy Search by MINIMIZING '{metric_name}' (Index {diversity_metric_index}) with Energy Constraint <= {energy_constraint:.3f} kWh...\")\n",
    "    startTime = timeit.default_timer()\n",
    "    \n",
    "    max_diversity_filtered_team_list = []\n",
    "    num_models = max(team_size_list) if team_size_list.size > 0 else 0\n",
    "\n",
    "    # Map team sizes to lists of (team, diversity_score)\n",
    "    teams_by_size = {size: [] for size in range(2, num_models + 1)}\n",
    "    for team, size in zip(all_teams, team_size_list):\n",
    "         team_key = ''.join(map(str, team))\n",
    "         if team_key in team_diversity_dict:\n",
    "             diversity_score = team_diversity_dict[team_key][diversity_metric_index]\n",
    "             teams_by_size[size].append((team, diversity_score))\n",
    "\n",
    "    current_best_team = None\n",
    "    for i in range(2, num_models + 1):\n",
    "        potential_teams_this_size = []\n",
    "        # Filter teams of current size that satisfy energy constraint and contain previous best team (if any)\n",
    "        for team, diversity_score in teams_by_size[i]:\n",
    "            if is_team_within_energy_constraint(team, energy_profile_dict, energy_constraint):\n",
    "                if current_best_team is None or set(current_best_team).issubset(set(team)):\n",
    "                     potential_teams_this_size.append((team, diversity_score))\n",
    "        \n",
    "        if not potential_teams_this_size:\n",
    "            # print(f\"  No suitable teams found for size {i}. Stopping greedy search.\")\n",
    "            break # Stop if no valid team can be found for this size\n",
    "            \n",
    "        # Find the team with the minimum diversity score among potential teams\n",
    "        best_team_this_size = min(potential_teams_this_size, key=lambda x: x[1] if not np.isnan(x[1]) else float('inf'))\n",
    "        \n",
    "        # Check if a valid team was found (diversity score is not inf)\n",
    "        if not np.isinf(best_team_this_size[1]):\n",
    "             current_best_team = best_team_this_size[0]\n",
    "             max_diversity_filtered_team_list.append(current_best_team)\n",
    "             \n",
    "    endTime = timeit.default_timer()\n",
    "    print(f\"Finished greedy search. Time: {endTime - startTime:.2f}s\")\n",
    "    print(f\"  Selected {len(max_diversity_filtered_team_list)} teams.\")\n",
    "    \n",
    "    return max_diversity_filtered_team_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_diversity_thresholds(diversity_scores_list, diversity_metrics_list):\n",
    "    \"\"\"Calculates the mean threshold for each diversity metric.\"\"\"\n",
    "    print(\"\\nCalculating mean diversity thresholds...\")\n",
    "    thresholds = {}\n",
    "    if diversity_scores_list.size == 0:\n",
    "        print(\"  Warning: Diversity scores list is empty. Cannot calculate thresholds.\")\n",
    "        return {dm: np.nan for dm in diversity_metrics_list}\n",
    "        \n",
    "    for j, dm in enumerate(diversity_metrics_list):\n",
    "        mean_value = np.nanmean(diversity_scores_list[..., j])\n",
    "        thresholds[dm] = mean_value\n",
    "        print(f\"  Threshold for {dm}: {mean_value:.4f}\")\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72958211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search_threshold(all_teams, team_size_list, team_diversity_dict, energy_profile_dict, \n",
    "                            energy_constraint, diversity_metric_index, thresholds, diversity_metrics_list):\n",
    "    \"\"\"Performs greedy search selecting teams under a diversity threshold.\"\"\"\n",
    "    # NOTE: The diversity is normalized to have lower score with higher diversity\n",
    "    metric_name = diversity_metrics_list[diversity_metric_index]\n",
    "    threshold = thresholds.get(metric_name, float('inf')) # Get threshold for the specific metric\n",
    "    print(f\"\\nPerforming Greedy Search using Threshold for '{metric_name}' (<= {threshold:.4f}) with Energy Constraint <= {energy_constraint:.3f} kWh...\")\n",
    "    startTime = timeit.default_timer()\n",
    "    \n",
    "    threshold_filtered_team_list = []\n",
    "    num_models = max(team_size_list) if team_size_list.size > 0 else 0\n",
    "\n",
    "    # Map team sizes to lists of (team, diversity_score)\n",
    "    teams_by_size = {size: [] for size in range(2, num_models + 1)}\n",
    "    for team, size in zip(all_teams, team_size_list):\n",
    "         team_key = ''.join(map(str, team))\n",
    "         if team_key in team_diversity_dict:\n",
    "             diversity_score = team_diversity_dict[team_key][diversity_metric_index]\n",
    "             teams_by_size[size].append((team, diversity_score))\n",
    "\n",
    "    processed_teams_this_round = set()\n",
    "    all_selected_teams = set()\n",
    "\n",
    "    for i in range(2, num_models + 1):\n",
    "        teams_under_threshold_this_size = []\n",
    "        current_round_added_tuples = set()\n",
    "\n",
    "        for team, diversity_score in teams_by_size[i]:\n",
    "            team_tuple = tuple(sorted(team)) # Use sorted tuple for set operations\n",
    "            # Check energy constraint, diversity threshold, and if it extends a previously selected team\n",
    "            if is_team_within_energy_constraint(team, energy_profile_dict, energy_constraint) and \\\n",
    "               not np.isnan(diversity_score) and diversity_score <= threshold and \\\n",
    "               (i == 2 or any(set(prev_team).issubset(set(team)) for prev_team in processed_teams_this_round)): # Check against teams added in the *previous* size\n",
    "                \n",
    "                teams_under_threshold_this_size.append(team)\n",
    "                current_round_added_tuples.add(team_tuple)\n",
    "                all_selected_teams.add(team_tuple)\n",
    "                \n",
    "        # Update the set of teams added in this round for the next iteration's check\n",
    "        processed_teams_this_round = {tuple(sorted(t)) for t in teams_under_threshold_this_size}\n",
    "        # print(f\"  Size {i}: Found {len(teams_under_threshold_this_size)} teams under threshold.\")\n",
    "        \n",
    "        # Break if no teams are found for a size\n",
    "        if not teams_under_threshold_this_size and i > 2:\n",
    "            print(f\"  No teams found for size {i} extending previous selections. Stopping.\")\n",
    "            break\n",
    "            \n",
    "    # Convert final set of tuples back to list of lists/tuples if needed\n",
    "    threshold_filtered_team_list = [list(t) for t in all_selected_teams]\n",
    "            \n",
    "    endTime = timeit.default_timer()\n",
    "    print(f\"  Selected {len(threshold_filtered_team_list)} teams.\")\n",
    "    \n",
    "    return threshold_filtered_team_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c78726",
   "metadata": {},
   "source": [
    "## 5. Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e94377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Predictions and Calculate Individual Accuracies\n",
    "predictionVectorsList, labelVectors, individualAccuracies, modelLoadTimes = load_predictions(\n",
    "    PREDICTION_DIR, MODELS, PREDICTION_SUFFIX, DEVICE\n",
    ")\n",
    "\n",
    "# Check if loading was successful\n",
    "if predictionVectorsList is None:\n",
    "    raise RuntimeError(\"Failed to load prediction data. Please check paths and file contents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calculate Team Accuracies (Exhaustive Soft Voting)\n",
    "teamAccuracyDict, allTeamsList = calculate_team_accuracies(\n",
    "    predictionVectorsList, labelVectors, len(MODELS)\n",
    ")\n",
    "\n",
    "# Analyze overall team accuracies (optional)\n",
    "analyze_team_accuracies(allTeamsList, teamAccuracyDict, \"Overall Team Accuracy Analysis (All Teams >= 2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculate Disagreement Samples\n",
    "sampleID, sampleTarget, disagreePredictions, disagreePredVectors = calculate_disagreement_samples(\n",
    "    predictionVectorsList, labelVectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Calculate Diversity Scores\n",
    "diversityScoresList, teamSizeList, processedTeamList = calculate_diversity_scores(\n",
    "    allTeamsList, \n",
    "    sampleID, \n",
    "    sampleTarget, \n",
    "    disagreePredictions, \n",
    "    disagreePredVectors, \n",
    "    DIVERSITY_METRICS_LIST,\n",
    "    cross_validation=CROSS_VALIDATION,\n",
    "    cross_validation_times=CROSS_VALIDATION_TIMES,\n",
    "    n_random_samples=N_RANDOM_SAMPLES\n",
    ")\n",
    "\n",
    "# 5. Create Team-to-Diversity Dictionary\n",
    "teamDiversityDict = create_team_diversity_dict(processedTeamList, diversityScoresList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4484304",
   "metadata": {},
   "source": [
    "## 6. Ensemble Selection Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a. Exhaustive Search with Energy Constraint\n",
    "exhaustiveFilteredTeams, exhaustiveBestTeamKey, exhaustiveMaxAcc = exhaustive_search_constrained(\n",
    "    allTeamsList, # Use the full list of teams\n",
    "    teamAccuracyDict,\n",
    "    ENERGY_PROFILES,\n",
    "    ENERGY_CONSTRAINT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35402d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b. Greedy Search by Maximizing/Minimizing Diversity with Energy Constraint\n",
    "# Ensure teamDiversityDict is not None before proceeding\n",
    "for GREEDY_DIVERSITY_METRIC_INDEX, diversity_metric in enumerate(DIVERSITY_METRICS_LIST):\n",
    "    greedyMaxDivTeams = greedy_search_max_diversity(\n",
    "        processedTeamList, # Use the list corresponding to diversity scores\n",
    "        teamSizeList, \n",
    "        teamDiversityDict,\n",
    "        ENERGY_PROFILES,\n",
    "        ENERGY_CONSTRAINT,\n",
    "        GREEDY_DIVERSITY_METRIC_INDEX, # Index for the metric (e.g., KW)\n",
    "        DIVERSITY_METRICS_LIST\n",
    "    )\n",
    "    \n",
    "    # Analyze the results of the greedy search\n",
    "    analyze_team_accuracies(\n",
    "        greedyMaxDivTeams, \n",
    "        teamAccuracyDict, \n",
    "        description=f\"Greedy Search (Minimizing {DIVERSITY_METRICS_LIST[GREEDY_DIVERSITY_METRIC_INDEX]}) Constrained\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nSkipping Greedy Max Diversity Search: teamDiversityDict not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c. Greedy Search using Diversity Threshold with Energy Constraint\n",
    "\n",
    "# First, calculate thresholds\n",
    "meanThresholds = calculate_mean_diversity_thresholds(diversityScoresList, DIVERSITY_METRICS_LIST)\n",
    "\n",
    "for GREEDY_DIVERSITY_METRIC_INDEX, diversity_metric in enumerate(DIVERSITY_METRICS_LIST):\n",
    "    greedyThresholdTeams = greedy_search_threshold(\n",
    "        processedTeamList, # Use the list corresponding to diversity scores\n",
    "        teamSizeList,\n",
    "        teamDiversityDict,\n",
    "        ENERGY_PROFILES,\n",
    "        ENERGY_CONSTRAINT,\n",
    "        GREEDY_DIVERSITY_METRIC_INDEX, # Index for the metric (e.g., KW)\n",
    "        meanThresholds, # Pass the calculated thresholds\n",
    "        DIVERSITY_METRICS_LIST\n",
    "    )\n",
    "        \n",
    "        # Analyze the results\n",
    "    analyze_team_accuracies(\n",
    "        greedyThresholdTeams, \n",
    "        teamAccuracyDict, \n",
    "        description=f\"Greedy Search (Threshold {DIVERSITY_METRICS_LIST[GREEDY_DIVERSITY_METRIC_INDEX]} <= {meanThresholds.get(DIVERSITY_METRICS_LIST[GREEDY_DIVERSITY_METRIC_INDEX], 'N/A'):.4f}) Constrained\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
